{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BirdCLEF 2025 Competition: Bird Song Classification\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook explores the BirdCLEF 2025 competition, a machine learning challenge focused on bird song classification. The competition is hosted on Kaggle and aims to develop algorithms that can identify bird species from audio recordings.\n",
    "\n",
    "Bird song classification is a challenging task with important applications in biodiversity monitoring, conservation efforts, and ecological research. Automated identification systems can help researchers process large volumes of audio data collected in the field, enabling more efficient and comprehensive studies of bird populations and behavior.\n",
    "\n",
    "### Competition Overview\n",
    "- **Goal**: Classify bird songs into one of 2000+ species\n",
    "- **Dataset**: Audio recordings of bird vocalizations with species labels\n",
    "- **Evaluation**: Models will be assessed on their ability to correctly identify bird species from audio samples\n",
    "- **Competition Link**: [BirdCLEF 2025 on Kaggle](https://www.kaggle.com/competitions/birdclef-2025/overview)\n",
    "\n",
    "Let's begin by exploring the dataset structure and understanding the nature of the bird song recordings we'll be working with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files\n",
    "**train_audio/** The training data consists of short recordings of individual bird, amphibian, mammal and insects sounds generously uploaded by users of xeno-canto.org, iNaturalist and the Colombian Sound Archive (CSA) of the Humboldt Institute for Biological Resources Research in Colombia. These files have been resampled to 32 kHz where applicable to match the test set audio and converted to the `ogg` format. Filenames consist of `[collection][file_id_in_collection].ogg`. The training data should have nearly all relevant files; we expect there is no benefit to looking for more on xeno-canto.org or iNaturalist and appreciate your cooperation in limiting the burden on their servers. If you do, please make sure to adhere to the scraping rules of these data portals.\n",
    "\n",
    "**test_soundscapes/** When you submit a notebook, the **test_soundscapes** directory will be populated with approximately 700 recordings to be used for scoring. They are 1 minute long and in `ogg` audio format, resampled to 32 kHz. The file names are randomized, but have the general form of `soundscape_xxxxxx.ogg`. It should take your submission notebook approximately five minutes to load all the test soundscapes. Not all species from the train data actually occur in the test data.\n",
    "\n",
    "**train_soundscapes/** Unlabeled audio data from the same recording locations as the test soundscapes. Filenames consist of `[site]_[date]_[local_time].ogg`; although recorded at the same location, precise recording sites of unlabeled soundscapes do NOT overlap with recording sites of the hidden test data.\n",
    "\n",
    "**train.csv** A wide range of metadata is provided for the training data. The most directly relevant fields are:\n",
    "\n",
    "- `primary_label`: A code for the species (eBird code for birds, iNaturalist taxon ID for non-birds). You can review detailed information about the species by appending codes to eBird and iNaturalis taxon URL, such as `https://ebird.org/species/gretin1` for the Great Tinamou or `https://www.inaturalist.org/taxa/24322` for the Red Snouted Tree Frog. Not all species have their own pages; some links might fail.\n",
    "- `secondary_labels`: List of species labels that have been marked by recordists to also occur in the recording. Can be incomplete.\n",
    "- `latitude` & `longitude`: Coordinates for where the recording was taken. Some bird species may have local call 'dialects,' so you may want to seek geographic diversity in your training data.\n",
    "- `author`: The user who provided the recording. Unknown if no name was provided.\n",
    "- `filename`: The name of the associated audio file.\n",
    "- `rating`: Values in 1..5 (1 - low quality, 5 - high quality) provided by users of Xeno-canto; 0 implies no rating is available; iNaturalist and the CSA do not provide quality ratings.\n",
    "- `collection`: Either `XC`, `iNat` or `CSA`, indicating which collection the recording was taken from. Filenames also reference the collection and the ID within that collection.\n",
    "\n",
    "**sample_submission.csv** A valid sample submission.\n",
    "\n",
    "- `row_id`: A slug of `soundscape_[soundscape_id]_[end_time]` for the prediction; e.g., Segment 00:15-00:20 of 1-minute test soundscape `soundscape_12345.ogg` has row ID `soundscape_12345_20`.\n",
    "- `[species_id]`: There are 206 species ID columns. You will need to predict the probability of the presence of each species for each row.\n",
    "\n",
    "**taxonomy.csv** - Data on the different species, including iNaturalist taxon ID and class name (Aves, Amphibia, Mammalia, Insecta).\n",
    "\n",
    "**recording_location.txt** - Some high-level information on the recording location (El Silencio Natural Reserve)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a nutshell, here is what we will do with the training data:\n",
    "\n",
    "1. **train_audio/**\n",
    "   - Contains individual, labeled bird sounds\n",
    "   - These are clean, single-species recordings\n",
    "   - Primary use: This will be our main training data for learning species-specific features\n",
    "1. **train_soundscapes/**\n",
    "   - Contains full 1-minute recordings from actual environments\n",
    "   - Contains background noise, multiple species\n",
    "   - Similar to the test data format\n",
    "   - Primary use: Fine-tuning and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as AT\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the [BirdNET paper](https://www.sciencedirect.com/science/article/pii/S1574954121000273), we can extract the following key insights:\n",
    "1. Spectrogram Parameters:\n",
    "   - Using mel-spectrograms with 64 bands\n",
    "   - Frequency range: 150 Hz to 15 kHz\n",
    "   - FFT window size adjusted for 32kHz sampling rate\n",
    "   - 25% overlap between frames\n",
    "2. Signal Processing:\n",
    "   - 3-second chunks for processing\n",
    "   - Signal strength-based detection for extracting relevant segments\n",
    "   - Log scaling for magnitude (better for noisy environments)\n",
    "3. Data Augmentation:\n",
    "   - Pitch shifting within the frequency range\n",
    "   - Temporal shifting within the 3-second window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdSongPreprocessor:\n",
    "    def __init__(self):\n",
    "        # Key parameters from the paper:\n",
    "        self.sample_rate = 32000  # Competition data is 32kHz\n",
    "        self.n_fft = 1024  # FFT window size (~32ms at 32kHz)\n",
    "        self.hop_length = 256  # 25% overlap as mentioned in BirdNET paper\n",
    "        self.f_min = 150  # Min frequency 150 Hz\n",
    "        self.f_max = 15000  # Max frequency 15 kHz\n",
    "        self.n_mels = 64  # 64 mel bands\n",
    "        \n",
    "        # Initialize mel spectrogram transformer\n",
    "        self.mel_spectrogram = AT.MelSpectrogram(\n",
    "            sample_rate=self.sample_rate,\n",
    "            n_fft=self.n_fft,\n",
    "            win_length=self.n_fft,\n",
    "            hop_length=self.hop_length,\n",
    "            f_min=self.f_min,\n",
    "            f_max=self.f_max,\n",
    "            n_mels=self.n_mels,\n",
    "            mel_scale=\"htk\",  # Using HTK-style mel scaling\n",
    "            power=2.0,  # Power spectrogram\n",
    "            normalized=True,\n",
    "            norm='slaney'  # Slaney-style mel normalization\n",
    "        )\n",
    "\n",
    "    def extract_signal_segments(self, waveform, threshold_factor=3.0, noise_threshold_factor=2.5):\n",
    "        \"\"\"\n",
    "        Extract segments containing bird vocalizations based on signal strength\n",
    "        Implementation based on Sprengel et al., 2016 approach\n",
    "        \n",
    "        Args:\n",
    "            waveform: Input audio waveform\n",
    "            threshold_factor: Factor for signal detection (3.0 as in paper)\n",
    "            noise_threshold_factor: Factor for noise detection (2.5 as in paper)\n",
    "            \n",
    "        Returns:\n",
    "            signal_mask: Boolean mask indicating signal segments\n",
    "            noise_mask: Boolean mask indicating noise segments\n",
    "        \"\"\"\n",
    "        # Convert to spectrogram without log scaling for signal detection\n",
    "        spec = self.mel_spectrogram(waveform)\n",
    "        \n",
    "        # Normalize spectrogram to [0,1] range as described in the paper\n",
    "        spec_norm = spec / (torch.max(spec) + 1e-9)\n",
    "        \n",
    "        # Calculate row (frequency) and column (time) medians\n",
    "        freq_medians = torch.median(spec_norm, dim=2, keepdim=True).values\n",
    "        time_medians = torch.median(spec_norm, dim=1, keepdim=True).values\n",
    "        \n",
    "        # Select pixels that are N times bigger than both medians\n",
    "        # For signal parts (threshold_factor = 3.0)\n",
    "        signal_pixels = (spec_norm > (threshold_factor * freq_medians)) & \\\n",
    "                        (spec_norm > (threshold_factor * time_medians))\n",
    "        \n",
    "        # For noise parts (noise_threshold_factor = 2.5)\n",
    "        noise_pixels = (spec_norm > (noise_threshold_factor * freq_medians)) & \\\n",
    "                    (spec_norm > (noise_threshold_factor * time_medians)) & \\\n",
    "                    ~signal_pixels  # Ensure no overlap with signal\n",
    "        \n",
    "        # Apply binary erosion and dilation to clean up the masks\n",
    "        # First convert to numpy for morphological operations\n",
    "        signal_pixels_np = signal_pixels[0].cpu().numpy()\n",
    "        noise_pixels_np = noise_pixels[0].cpu().numpy()\n",
    "        \n",
    "        # Define kernel for morphological operations (4x4 as in paper)\n",
    "        kernel = np.ones((4, 4), np.uint8)\n",
    "        \n",
    "        # Apply erosion followed by dilation (opening operation)\n",
    "        from scipy import ndimage\n",
    "        signal_pixels_np = ndimage.binary_erosion(signal_pixels_np, structure=kernel)\n",
    "        signal_pixels_np = ndimage.binary_dilation(signal_pixels_np, structure=kernel)\n",
    "        \n",
    "        noise_pixels_np = ndimage.binary_erosion(noise_pixels_np, structure=kernel)\n",
    "        noise_pixels_np = ndimage.binary_dilation(noise_pixels_np, structure=kernel)\n",
    "        \n",
    "        # Create indicator vectors (1 if column contains at least one 1)\n",
    "        signal_indicator = np.any(signal_pixels_np, axis=0).astype(np.uint8)\n",
    "        noise_indicator = np.any(noise_pixels_np, axis=0).astype(np.uint8)\n",
    "        \n",
    "        # Apply dilation to smooth the indicator vectors\n",
    "        dilation_kernel = np.ones(4)\n",
    "        signal_indicator = ndimage.binary_dilation(signal_indicator, structure=dilation_kernel)\n",
    "        signal_indicator = ndimage.binary_dilation(signal_indicator, structure=dilation_kernel)\n",
    "        \n",
    "        noise_indicator = ndimage.binary_dilation(noise_indicator, structure=dilation_kernel)\n",
    "        noise_indicator = ndimage.binary_dilation(noise_indicator, structure=dilation_kernel)\n",
    "        \n",
    "        # Ensure no overlap between signal and noise\n",
    "        noise_indicator = noise_indicator & ~signal_indicator\n",
    "        \n",
    "        # Convert back to torch tensors\n",
    "        signal_mask = torch.from_numpy(signal_indicator).to(waveform.device)\n",
    "        noise_mask = torch.from_numpy(noise_indicator).to(waveform.device)\n",
    "        \n",
    "        # Scale masks to match waveform length\n",
    "        # Calculate scaling factor\n",
    "        spec_time_bins = signal_mask.shape[0]\n",
    "        waveform_length = waveform.shape[1]\n",
    "        scaling_factor = waveform_length / spec_time_bins\n",
    "        \n",
    "        # Create full-length masks\n",
    "        full_signal_mask = torch.zeros(waveform_length, device=waveform.device, dtype=torch.bool)\n",
    "        full_noise_mask = torch.zeros(waveform_length, device=waveform.device, dtype=torch.bool)\n",
    "        \n",
    "        # Map each spectrogram time bin to corresponding audio samples\n",
    "        for i in range(spec_time_bins):\n",
    "            start_idx = int(i * scaling_factor)\n",
    "            end_idx = int((i + 1) * scaling_factor)\n",
    "            if signal_mask[i]:\n",
    "                full_signal_mask[start_idx:end_idx] = True\n",
    "            if noise_mask[i]:\n",
    "                full_noise_mask[start_idx:end_idx] = True\n",
    "        \n",
    "        return full_signal_mask, full_noise_mask\n",
    "\n",
    "    def separate_signal_noise(self, waveform):\n",
    "        \"\"\"\n",
    "        Separate audio into signal (bird vocalization) and noise parts\n",
    "        \n",
    "        Args:\n",
    "            waveform: Input audio waveform\n",
    "            \n",
    "        Returns:\n",
    "            signal_waveform: Audio containing only bird vocalizations\n",
    "            noise_waveform: Audio containing only background noise\n",
    "        \"\"\"\n",
    "        # Get signal and noise masks\n",
    "        signal_mask, noise_mask = self.extract_signal_segments(waveform)\n",
    "        \n",
    "        # Create signal and noise waveforms\n",
    "        signal_waveform = torch.zeros_like(waveform)\n",
    "        noise_waveform = torch.zeros_like(waveform)\n",
    "        \n",
    "        # Apply masks\n",
    "        signal_waveform[:, signal_mask] = waveform[:, signal_mask]\n",
    "        noise_waveform[:, noise_mask] = waveform[:, noise_mask]\n",
    "        \n",
    "        return signal_waveform, noise_waveform\n",
    "\n",
    "    def process_audio(self, audio_path, chunk_duration=3.0, overlap=0.5):\n",
    "        \"\"\"\n",
    "        Process audio file into mel spectrograms, cutting into equal-sized chunks as\n",
    "        described in Sprengel et al., using 3-second chunks as recommended in Kahl et al.\n",
    "        \n",
    "        Args:\n",
    "            audio_path: Path to audio file\n",
    "            chunk_duration: Duration of each chunk in seconds (default: 3.0s)\n",
    "            overlap: Overlap between chunks as a fraction (default: 0.5 = 50%)\n",
    "            \n",
    "        Returns:\n",
    "            signal_chunks: List of spectrograms from signal parts\n",
    "            noise_chunks: List of spectrograms from noise parts\n",
    "        \"\"\"\n",
    "        # Load audio\n",
    "        waveform, sr = torchaudio.load(audio_path)\n",
    "        \n",
    "        # Resample if necessary\n",
    "        if sr != self.sample_rate:\n",
    "            resampler = AT.Resample(sr, self.sample_rate)\n",
    "            waveform = resampler(waveform)\n",
    "        \n",
    "        # Separate signal and noise\n",
    "        signal_waveform, noise_waveform = self.separate_signal_noise(waveform)\n",
    "        \n",
    "        # Calculate chunk parameters\n",
    "        chunk_samples = int(chunk_duration * self.sample_rate)\n",
    "        hop_samples = int(chunk_samples * (1 - overlap))\n",
    "        \n",
    "        # Process signal chunks\n",
    "        signal_chunks = []\n",
    "        for start in range(0, signal_waveform.shape[1] - chunk_samples + 1, hop_samples):\n",
    "            # Extract chunk\n",
    "            chunk = signal_waveform[:, start:start + chunk_samples]\n",
    "            \n",
    "            # Skip chunks with no signal\n",
    "            if torch.sum(chunk) > 0:\n",
    "                # Convert to spectrogram\n",
    "                spec = self.mel_spectrogram(chunk)\n",
    "                spec = torch.log(spec + 1e-9)  # Log scaling\n",
    "                signal_chunks.append(spec)\n",
    "        \n",
    "        # Process noise chunks\n",
    "        noise_chunks = []\n",
    "        for start in range(0, noise_waveform.shape[1] - chunk_samples + 1, hop_samples):\n",
    "            # Extract chunk\n",
    "            chunk = noise_waveform[:, start:start + chunk_samples]\n",
    "            \n",
    "            # Skip chunks with no noise\n",
    "            if torch.sum(chunk) > 0:\n",
    "                # Convert to spectrogram\n",
    "                spec = self.mel_spectrogram(chunk)\n",
    "                spec = torch.log(spec + 1e-9)  # Log scaling\n",
    "                noise_chunks.append(spec)\n",
    "        \n",
    "        return signal_chunks, noise_chunks\n",
    "    \n",
    "    def augment_spectrogram(self, spec, noise_specs=None):\n",
    "        \"\"\"\n",
    "        Apply domain-specific augmentations to spectrograms as described in BirdNET paper\n",
    "\n",
    "        Args:\n",
    "            spec (torch.Tensor): Input spectrogram\n",
    "            noise_specs (list): List of noise spectrograms from non-salient chunks\n",
    "        \"\"\"\n",
    "        # Maximum of three augmentations per sample as mentioned in the paper\n",
    "        num_augmentations = np.random.randint(1, 4)\n",
    "        augmented = spec.clone()\n",
    "\n",
    "        # List of possible augmentations\n",
    "        augmentations = [\n",
    "            self._frequency_shift,\n",
    "            self._time_shift,\n",
    "            self._spec_warp,\n",
    "            lambda x: self._add_ambient_noise(x, noise_specs) if noise_specs else x,\n",
    "        ]\n",
    "\n",
    "        # Randomly select and apply augmentations\n",
    "        selected_augs = np.random.choice(\n",
    "            augmentations, size=num_augmentations, replace=False\n",
    "        )\n",
    "\n",
    "        for aug in selected_augs:\n",
    "            if np.random.random() > 0.5:  # 0.5 probability as mentioned in paper\n",
    "                augmented = aug(augmented)\n",
    "\n",
    "        return augmented\n",
    "\n",
    "    def _frequency_shift(self, spec, max_shift=10):\n",
    "        \"\"\"Vertical roll - Shift in frequency domain\"\"\"\n",
    "        shift = np.random.randint(-max_shift, max_shift)\n",
    "        return torch.roll(spec, shifts=shift, dims=1)\n",
    "\n",
    "    def _time_shift(self, spec, max_shift=50):\n",
    "        \"\"\"Horizontal roll - Shift in time domain\"\"\"\n",
    "        shift = np.random.randint(-max_shift, max_shift)\n",
    "        return torch.roll(spec, shifts=shift, dims=2)\n",
    "\n",
    "    def _spec_warp(self, spec):\n",
    "        \"\"\"\n",
    "        Spectrogram warping similar to SpecAugment\n",
    "        Applies random partial stretching in time and frequency\n",
    "        \"\"\"\n",
    "        freq_dim, time_dim = spec.shape[1:]\n",
    "\n",
    "        # Create warping parameters\n",
    "        w = np.random.randint(5, 20)  # window size\n",
    "        center_freq = np.random.randint(w, freq_dim - w)\n",
    "        center_time = np.random.randint(w, time_dim - w)\n",
    "\n",
    "        # Create warping matrix\n",
    "        factor = np.random.uniform(0.8, 1.2)\n",
    "        warped = spec.clone()\n",
    "\n",
    "        # Apply warping around center point\n",
    "        warped[\n",
    "            :, center_freq - w : center_freq + w, center_time - w : center_time + w\n",
    "        ] *= factor\n",
    "\n",
    "        return warped\n",
    "\n",
    "    def _add_ambient_noise(self, spec, noise_specs, max_weight=0.5):\n",
    "        \"\"\"\n",
    "        Augment signal spectrogram with noise as described in Sprengel et al.\n",
    "\n",
    "        Args:\n",
    "            spec: Signal spectrogram to augment\n",
    "            noise_specs: List of noise spectrograms to choose from\n",
    "            max_weight: Maximum weight for noise addition\n",
    "\n",
    "        Returns:\n",
    "            Augmented spectrogram\n",
    "        \"\"\"\n",
    "        if not noise_specs:\n",
    "            return spec\n",
    "\n",
    "        # Randomly select a noise spectrogram\n",
    "        noise_spec = noise_specs[np.random.randint(len(noise_specs))]\n",
    "\n",
    "        # Ensure shapes match\n",
    "        if noise_spec.shape != spec.shape:\n",
    "            # Resize noise spectrogram to match signal shape\n",
    "            _, freq_dim, time_dim = spec.shape\n",
    "\n",
    "            # Handle frequency dimension mismatch (shouldn't happen with same preprocessing)\n",
    "            if noise_spec.shape[1] != freq_dim:\n",
    "                # Interpolate frequency dimension\n",
    "                noise_spec = torch.nn.functional.interpolate(\n",
    "                    noise_spec, size=(freq_dim, noise_spec.shape[2]), mode=\"bilinear\"\n",
    "                )\n",
    "\n",
    "            # Handle time dimension mismatch\n",
    "            if noise_spec.shape[2] != time_dim:\n",
    "                # Center crop or pad\n",
    "                if noise_spec.shape[2] > time_dim:\n",
    "                    # Center crop\n",
    "                    start = (noise_spec.shape[2] - time_dim) // 2\n",
    "                    noise_spec = noise_spec[:, :, start : start + time_dim]\n",
    "                else:\n",
    "                    # Pad\n",
    "                    pad_size = time_dim - noise_spec.shape[2]\n",
    "                    pad_left = pad_size // 2\n",
    "                    pad_right = pad_size - pad_left\n",
    "                    noise_spec = torch.nn.functional.pad(\n",
    "                        noise_spec, (pad_left, pad_right)\n",
    "                    )\n",
    "\n",
    "        # Random weighting for noise (as in paper)\n",
    "        weight = np.random.uniform(0, max_weight)\n",
    "\n",
    "        # Add weighted noise\n",
    "        augmented = (1 - weight) * spec + weight * noise_spec\n",
    "\n",
    "        return augmented\n",
    "\n",
    "    def collect_ambient_noise(self, audio_path):\n",
    "        \"\"\"\n",
    "        Collect non-salient chunks for ambient noise augmentation\n",
    "        Using the improved signal/noise separation method\n",
    "        \"\"\"\n",
    "        waveform, sr = torchaudio.load(audio_path)\n",
    "        if sr != self.sample_rate:\n",
    "            resampler = AT.Resample(sr, self.sample_rate)\n",
    "            waveform = resampler(waveform)\n",
    "\n",
    "        # Separate signal and noise using our new method\n",
    "        _, noise_waveform = self.separate_signal_noise(waveform)\n",
    "        \n",
    "        # Check if we have any noise segments\n",
    "        if torch.sum(noise_waveform) > 0:\n",
    "            # Convert to spectrogram\n",
    "            noise_spec = self.mel_spectrogram(noise_waveform)\n",
    "            noise_spec = torch.log(noise_spec + 1e-9)\n",
    "            return noise_spec\n",
    "            \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor\n",
    "preprocessor = BirdSongPreprocessor()\n",
    "\n",
    "def prepare_batch(\n",
    "    audio_files, save_dir=\"train_audio_processed\", training=True, show_progress=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Prepare a batch of audio files for model training or inference.\n",
    "\n",
    "    Args:\n",
    "        audio_files (list): List of audio file paths\n",
    "        save_dir (str): Directory to save the processed audio files\n",
    "        training (bool): Whether to apply augmentation\n",
    "        show_progress (bool): Whether to show progress bars\n",
    "    \"\"\"\n",
    "    # Create save_dir if it doesn't exist\n",
    "    save_dir = Path(save_dir)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Create metadata file to store mapping\n",
    "    metadata = []\n",
    "    signal_specs = []\n",
    "    noise_specs = []\n",
    "\n",
    "    # Group files by folder for better progress tracking\n",
    "    files_by_folder = {}\n",
    "    for file in audio_files:\n",
    "        folder = os.path.basename(os.path.dirname(file))\n",
    "        if folder not in files_by_folder:\n",
    "            files_by_folder[folder] = []\n",
    "        files_by_folder[folder].append(file)\n",
    "\n",
    "    # Process audio files\n",
    "    folder_iter = tqdm(\n",
    "        files_by_folder.items(),\n",
    "        desc=\"Processing folders\",\n",
    "        disable=not show_progress,\n",
    "    )\n",
    "\n",
    "    for folder, folder_files in folder_iter:\n",
    "        # Create folder if it doesn't exist\n",
    "        (save_dir / folder).mkdir(exist_ok=True)\n",
    "\n",
    "        for audio_file in tqdm(\n",
    "            folder_files,\n",
    "            desc=f\"Processing {folder}\",\n",
    "            leave=False,\n",
    "            disable=not show_progress,\n",
    "        ):\n",
    "            # Create unique filenames for the processed specs\n",
    "            base_filename = Path(audio_file).stem\n",
    "            signal_dir = save_dir / folder / \"signal\"\n",
    "            noise_dir = save_dir / folder / \"noise\"\n",
    "\n",
    "            signal_dir.mkdir(exist_ok=True)\n",
    "            noise_dir.mkdir(exist_ok=True)\n",
    "\n",
    "            # Check if already processed\n",
    "            signal_pattern = str(signal_dir / f\"{base_filename}_*.pt\")\n",
    "            existing_signal_files = glob.glob(signal_pattern)\n",
    "\n",
    "            if existing_signal_files:\n",
    "                # Load existing spectrograms\n",
    "                for file in existing_signal_files:\n",
    "                    spec = torch.load(file)\n",
    "                    signal_specs.append(spec)\n",
    "                    metadata.append(\n",
    "                        {\n",
    "                            \"original_file\": audio_file,\n",
    "                            \"processed_file\": file,\n",
    "                            \"folder\": folder,\n",
    "                            \"type\": \"signal\",\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                # Load existing noise specs if available\n",
    "                noise_pattern = str(noise_dir / f\"{base_filename}_*.pt\")\n",
    "                for file in glob.glob(noise_pattern):\n",
    "                    spec = torch.load(file)\n",
    "                    noise_specs.append(spec)\n",
    "                    metadata.append(\n",
    "                        {\n",
    "                            \"original_file\": audio_file,\n",
    "                            \"processed_file\": file,\n",
    "                            \"folder\": folder,\n",
    "                            \"type\": \"noise\",\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Process audio file into chunks\n",
    "                signal_chunks, noise_chunks = preprocessor.process_audio(\n",
    "                    audio_file\n",
    "                )\n",
    "\n",
    "                # Augment signal chunks\n",
    "                signal_chunks = [preprocessor.augment_spectrogram(chunk, noise_chunks) for chunk in signal_chunks]\n",
    "\n",
    "                # Save signal chunks\n",
    "                for i, chunk in enumerate(signal_chunks):\n",
    "                    chunk_file = signal_dir / f\"{base_filename}_{i:03d}.pt\"\n",
    "                    torch.save(chunk, chunk_file)\n",
    "                    signal_specs.append(chunk)\n",
    "                    metadata.append(\n",
    "                        {\n",
    "                            \"original_file\": audio_file,\n",
    "                            \"processed_file\": str(chunk_file),\n",
    "                            \"folder\": folder,\n",
    "                            \"type\": \"signal\",\n",
    "                            \"chunk_index\": i,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                # Save noise chunks\n",
    "                for i, chunk in enumerate(noise_chunks):\n",
    "                    chunk_file = noise_dir / f\"{base_filename}_{i:03d}.pt\"\n",
    "                    torch.save(chunk, chunk_file)\n",
    "                    noise_specs.append(chunk)\n",
    "                    metadata.append(\n",
    "                        {\n",
    "                            \"original_file\": audio_file,\n",
    "                            \"processed_file\": str(chunk_file),\n",
    "                            \"folder\": folder,\n",
    "                            \"type\": \"noise\",\n",
    "                            \"chunk_index\": i,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError processing {audio_file}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    # Save metadata\n",
    "    metadata_df = pd.DataFrame(metadata)\n",
    "    metadata_df.to_csv(save_dir / \"metadata.csv\", index=False)\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\nProcessing Summary:\")\n",
    "    print(f\"Total signal chunks: {len(signal_specs)}\")\n",
    "    print(f\"Total noise chunks: {len(noise_specs)}\")\n",
    "    print(\"Files per folder:\")\n",
    "    folder_counts = metadata_df[metadata_df[\"type\"] == \"signal\"][\n",
    "        \"folder\"\n",
    "    ].value_counts()\n",
    "    print(folder_counts.head().to_string())\n",
    "\n",
    "    return signal_specs, noise_specs, metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28564 audio files\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9dda74cb8d84fafb676b229dd394eba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing folders:   0%|          | 0/206 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dcff235a54345fe84219539dfac8284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing crbtan1:   0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7089a19fc1d04ae990a59e21fe0b4147",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing 48124:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c92ea9fd82a4e5489d5d134b5c1bc48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing 476537:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de262c346fa346fb833a96a1a9d961c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing 66016:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b10536ffd2c4b03a181ce9c7ca97b0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing 42087:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4950ec8c48d24d02bdfa69b9e6df5238",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing crcwoo1:   0%|          | 0/144 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54479a4820a344338f66ec6c126728f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing blcant4:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "167103635ba1478d81a00f2a3904874b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing 787625:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c725331097304768a9f757ec4c4cdd3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing 24292:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f36cfd2e8ce348b2a7d4bcd7c5166e46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing 21116:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "585ef28d04f647d18549736a388b5aa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing 46010:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0630deb8dbe411b9809ac9d81ff7ed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing compau:   0%|          | 0/808 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get all .ogg files recursively\n",
    "train_files = glob.glob(\"data/train_audio/**/*.ogg\", recursive=True)\n",
    "\n",
    "print(f\"Found {len(train_files)} audio files\")\n",
    "\n",
    "train_specs = prepare_batch(train_files, training=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
